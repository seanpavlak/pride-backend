{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from glob import glob\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "from plotly import tools\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os.path as path\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer as count_vectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB as multinomial_nb\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.sparse import csr_matrix\n",
    "import string\n",
    "import time\n",
    "import operator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from wordcloud import WordCloud\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.layers import Activation, Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribute_data(yelp):\n",
    "    yelp_subset = []\n",
    "    \n",
    "    # 2 parties\n",
    "    # party 0 -> random set of 1000 reviews\n",
    "    # party 1 -> set of 1000 only 1 & 5 reviews \n",
    "    \n",
    "    # 10 parties\n",
    "    # party 0 - 5 -> random set of 1000 reviews\n",
    "    # party 6 -> set of 1000 only 1 & 2 reviews\n",
    "    # party 7 -> set of 1000 only 2 & 3 reviews\n",
    "    # party 8 -> set of 1000 only 3 & 4 reviews\n",
    "    # party 9 -> set of 1000 only 4 & 5 reviews\n",
    "    \n",
    "    # n parties\n",
    "    # party n -> random set of 1000 reviews\n",
    "    \n",
    "    party_0 = yelp.sample(1000)\n",
    "    \n",
    "    yelp_subset.append(yelp[(yelp['stars'] == 4)])\n",
    "    yelp_subset.append(yelp[(yelp['stars'] == 5)])\n",
    "    \n",
    "    yelp_subset = pd.concat(yelp_subset)\n",
    "    \n",
    "    party_1 = yelp_subset.sample(1000)\n",
    "    \n",
    "    return (party_0, party_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_process(text, weak_sentiment_word_list):\n",
    "    word_list = []\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    for word in nopunc.split():\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('english'):\n",
    "            if word not in weak_sentiment_word_list:\n",
    "                word_list.append(word.lower())\n",
    "        \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_dataset(yelp):\n",
    "    yelp_normalized = []\n",
    "    \n",
    "    yelp_1 = yelp[(yelp['stars'] == 1)]\n",
    "    yelp_2 = yelp[(yelp['stars'] == 2)]\n",
    "    yelp_3 = yelp[(yelp['stars'] == 3)]\n",
    "    yelp_4 = yelp[(yelp['stars'] == 4)]\n",
    "    yelp_5 = yelp[(yelp['stars'] == 5)]\n",
    "    \n",
    "    limiting_factor = min([len(yelp_1), len(yelp_2), len(yelp_3), len(yelp_4), len(yelp_5)])\n",
    "        \n",
    "    yelp_normalized.append(yelp_1.sample(limiting_factor))\n",
    "    yelp_normalized.append(yelp_2.sample(limiting_factor))\n",
    "    yelp_normalized.append(yelp_3.sample(limiting_factor))\n",
    "    yelp_normalized.append(yelp_4.sample(limiting_factor))\n",
    "    yelp_normalized.append(yelp_5.sample(limiting_factor))\n",
    "    \n",
    "    return pd.concat(yelp_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_string(yelp, rating):\n",
    "    yelp = yelp[(yelp['stars'] == rating)]\n",
    "    string = []\n",
    "    \n",
    "    for text in yelp['tokenized']:\n",
    "        for token in text:\n",
    "            string.append(token)\n",
    "    return pd.Series(string).str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_list(string_count, length):\n",
    "    word_list = []\n",
    "    count = 0\n",
    "    \n",
    "    for word in string_count:\n",
    "        if count < length:\n",
    "            count += 1\n",
    "            word_list.append(word[0])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_weak_sentiment_list(yelp):\n",
    "    weak_sentiment_word_list = []\n",
    "\n",
    "    yelp_negative_string = generate_string(yelp, 1)\n",
    "    yelp_positive_string = generate_string(yelp, 5)\n",
    "\n",
    "    positive_string_count = sorted(word_count(yelp_positive_string).items(), \n",
    "                                   key=operator.itemgetter(1), \n",
    "                                   reverse = True)\n",
    "    \n",
    "    negative_string_count = sorted(word_count(yelp_negative_string).items(), \n",
    "                                   key=operator.itemgetter(1), \n",
    "                                   reverse = True)\n",
    "    \n",
    "    length = int((len(positive_string_count) + len(negative_string_count)) * 0.001 / 2)\n",
    "    \n",
    "    positive_word_list = generate_list(positive_string_count, length)\n",
    "    negative_word_list = generate_list(negative_string_count, length)\n",
    "    \n",
    "    for word in positive_word_list:\n",
    "        if word in negative_word_list:\n",
    "            weak_sentiment_word_list.append(word)\n",
    "    return weak_sentiment_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_dataset(yelp):\n",
    "    weak_sentiment_list = []\n",
    "    \n",
    "    yelp['text'] = yelp['text'].astype(str)\n",
    "    yelp['length'] = yelp['text'].apply(len)\n",
    "    yelp['tokenized'] = yelp.apply(lambda row: text_process(row['text'], weak_sentiment_list), axis=1)\n",
    "    weak_sentiment_list = generate_weak_sentiment_list(yelp)\n",
    "    yelp['tokenized'] = yelp.apply(lambda row: text_process(row['text'], weak_sentiment_list), axis=1)\n",
    "    \n",
    "    return yelp, weak_sentiment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_class(yelp, boundary):\n",
    "    if not boundary:\n",
    "        yelp_class = yelp\n",
    "    else:\n",
    "        yelp_class = yelp[(yelp['stars'] == 1) | (yelp['stars'] == 5)]\n",
    "    \n",
    "    yelp_class.shape\n",
    "    \n",
    "    return yelp_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_X_y(yelp_class):\n",
    "    X_list = []\n",
    "    \n",
    "    X = yelp_class['tokenized']\n",
    "    y = yelp_class['stars']\n",
    "    \n",
    "    for item in X:\n",
    "        X = ' '.join(item)\n",
    "        X_list.append(X)\n",
    "    \n",
    "    return X_list, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_transformer(X):\n",
    "    bow_transformer = count_vectorizer(ngram_range=(1, 2)).fit(X)\n",
    "    X = bow_transformer.transform(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_count(str):\n",
    "    counts = dict()\n",
    "    words = str.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp = pd.read_csv('./dataset/review_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp = normalize_dataset(yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_parties = distribute_data(yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_parties_clean = []\n",
    "\n",
    "for party in yelp_parties:\n",
    "    yelp_parties_clean.append(clean_dataset(party))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
